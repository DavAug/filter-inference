{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of Gaussian model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import chi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pints\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data-generating distribution is\n",
    "$$\n",
    "    y \\sim \\mathcal{N}(\\cdot | \\mu = 1.3, \\sigma ^2 = 4).\n",
    "$$\n",
    "\n",
    "We will assume that the variance of the distribution is known. Together with\n",
    "the prior $\\mu \\sim \\mathcal{N}(\\cdot | \\mu _0 = 0, \\sigma _0^2 = 16)$, we\n",
    "can use the conjugacy to compute the posterior distribution of $\\mu $\n",
    "$$\n",
    "    p(\\mu | \\mathcal{D}) = \\mathcal{N}(\\cdot | \\mu ', \\sigma '^2 ),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\mu' = \\sigma '^2\n",
    "    \\left(\\frac{\\mu _0}{\\sigma _0^2} + \\frac{n\\bar{y}}{\\sigma ^2}\\right),\n",
    "    \\quad\n",
    "    \\sigma '^2 = \\frac{\\sigma ^ 2\\sigma _0^2}{\\sigma ^2 + n\\sigma _0^2}.\n",
    "$$\n",
    "Here, $n$ is the number of observations in the dataset\n",
    "$\\mathcal{D}=\\{y_j^{\\mathrm{obs}}\\}$ and $\\bar{y}$ denotes the empirical mean,\n",
    "$\\bar{y} = \\sum _j y_j^{\\mathrm{obs}} / n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "true_parameters = [1.3, 2]\n",
    "data = np.random.normal(\n",
    "    loc=true_parameters[0], scale=true_parameters[1], size=1000)\n",
    "\n",
    "sns.displot(data, kind='kde', rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define log-likelihoods\n",
    "\n",
    "1. Exact Gaussian log-likelihood\n",
    "2. Naive Gaussian filter log-likelihood (stochastic)\n",
    "3. Gaussian filter log-likelihood (deterministic)\n",
    "4. Naive KDE filter log-likelihood (stochastic)\n",
    "5. KDE filter log-likelihood (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianLogLikelihood(chi.LogLikelihood):\n",
    "    def __init__(self, observations):\n",
    "        self._observations = observations\n",
    "\n",
    "    def __call__(self, parameters):\n",
    "        mean = parameters[0]\n",
    "        sigma = 2\n",
    "        score = np.sum(\n",
    "            scipy.stats.norm(loc=mean, scale=sigma).logpdf(self._observations))\n",
    "\n",
    "        return score\n",
    "\n",
    "    def n_parameters(self):\n",
    "        return 1\n",
    "\n",
    "    def get_parameter_names(self):\n",
    "        return ['Mean']\n",
    "\n",
    "    def get_id(self):\n",
    "        return [None] * self.n_parameters()\n",
    "\n",
    "\n",
    "class NaiveGaussianFilterLogLikelihood(chi.LogLikelihood):\n",
    "    def __init__(self, observations, n_samples=1000):\n",
    "        self._observations = observations\n",
    "        self._n_samples = int(n_samples)\n",
    "\n",
    "    def __call__(self, parameters):\n",
    "        # Sample from population distribution\n",
    "        mean = parameters[0]\n",
    "        sigma = 2\n",
    "        samples = np.random.normal(\n",
    "            loc=mean, scale=sigma, size=self._n_samples)\n",
    "\n",
    "        # Estimate mean and std from samples\n",
    "        mean = np.mean(samples)\n",
    "        sigma = np.std(samples, ddof=1)\n",
    "        score = np.sum(\n",
    "            scipy.stats.norm(loc=mean, scale=sigma).logpdf(self._observations))\n",
    "\n",
    "        return score\n",
    "\n",
    "    def n_parameters(self):\n",
    "        return 1\n",
    "\n",
    "    def get_parameter_names(self):\n",
    "        return ['Mean']\n",
    "\n",
    "    def get_id(self):\n",
    "        return [None] * self.n_parameters()\n",
    "\n",
    "\n",
    "class GaussianFilterLogLikelihood(chi.LogLikelihood):\n",
    "    def __init__(self, observations, n_samples=1000):\n",
    "        self._observations = observations\n",
    "        self._n_samples = int(n_samples)\n",
    "        self._normal = chi.GaussianErrorModel()\n",
    "\n",
    "    def __call__(self, parameters):\n",
    "        standard_samples = parameters[:self._n_samples]\n",
    "        mean = parameters[-1]\n",
    "        sigma = 2\n",
    "        samples = mean + standard_samples * sigma\n",
    "\n",
    "        # Estimate mean and std from samples\n",
    "        mean = np.mean(samples)\n",
    "        sigma = np.std(samples, ddof=1)\n",
    "        score = np.sum(\n",
    "            scipy.stats.norm(loc=mean, scale=sigma).logpdf(self._observations))\n",
    "\n",
    "        return score\n",
    "\n",
    "    def evaluateS1(self, parameters):\n",
    "        standard_samples = np.array(parameters[:self._n_samples])\n",
    "        mean = parameters[-1]\n",
    "        std = 2\n",
    "        samples = mean + standard_samples * std\n",
    "\n",
    "        # Estimate mean and std from samples\n",
    "        mu = np.mean(samples)\n",
    "        sigma = np.std(samples, ddof=1)\n",
    "\n",
    "        # Compute sensitivity of mean and sigma to input params\n",
    "        dmu_dsamples = std / self._n_samples\n",
    "        dmu_dmean = 1\n",
    "        dsigma_dsamples = \\\n",
    "            (samples - mu) * std / sigma / (self._n_samples - 1)\n",
    "        dsigma_dmean = 0\n",
    "\n",
    "        # Propagate sensitivies through likelihood\n",
    "        score, sens = self._normal.compute_sensitivities(\n",
    "            parameters=[sigma],\n",
    "            model_output=np.array([mu] * len(self._observations)),\n",
    "            model_sensitivities=np.broadcast_to(np.hstack([\n",
    "                dmu_dsamples, dmu_dmean]),\n",
    "                shape=(len(self._observations), 2)),\n",
    "            observations=self._observations)\n",
    "\n",
    "        # Collect sensitivities\n",
    "        # dp/dsamples = dp/dmu * dmu/dsample + dp/dsigma * dsigma/dsample\n",
    "        # dp/dmean = dp/dmu * dmu/dmean + dp/dsigma * dsigma/dmean\n",
    "        # dp/dstd = dp/dmu * dmu/dstd + dp/dsigma * dsigma/dstd\n",
    "        sensitivities = np.empty(shape=self.n_parameters())\n",
    "        sensitivities[:self._n_samples] = sens[0] + sens[2] * dsigma_dsamples\n",
    "        sensitivities[self._n_samples] = sens[1] + sens[2] * dsigma_dmean\n",
    "\n",
    "        return score, sensitivities\n",
    "\n",
    "    def n_parameters(self):\n",
    "        return self._n_samples + 1\n",
    "\n",
    "    def get_parameter_names(self):\n",
    "        return [\n",
    "            'Sample' for _ in range(self._n_samples)] + ['Mean']\n",
    "\n",
    "    def get_id(self):\n",
    "        return ['ID %d' % _id for _id in range(self._n_samples)] + [None]\n",
    "\n",
    "\n",
    "class NaiveKDEFilterLogLikelihood(chi.LogLikelihood):\n",
    "    def __init__(self, observations, n_samples=1000, kernel_scale=None):\n",
    "        self._observations = observations\n",
    "        self._n_obs = len(observations)\n",
    "        self._n_samples = int(n_samples)\n",
    "        self._normal = chi.GaussianErrorModel()\n",
    "\n",
    "        # Set kernel scale\n",
    "        # (If None or non-positive use Scott's rule of thumb (like scipy))\n",
    "        if (not kernel_scale) or (kernel_scale <=0):\n",
    "            kernel_scale = self._n_samples ** (-0.2)\n",
    "        self._kernel_scale = kernel_scale\n",
    "\n",
    "    def __call__(self, parameters):\n",
    "        mean = parameters[0]\n",
    "        sigma = 2\n",
    "        samples = np.random.normal(\n",
    "            loc=mean, scale=sigma, size=self._n_samples)\n",
    "\n",
    "        # Estimate log-likelihood as the mean across kdes\n",
    "        score = np.sum(scipy.special.logsumexp(\n",
    "            - np.log(2 * np.pi) / 2 - np.log(self._kernel_scale) - \\\n",
    "            (samples[np.newaxis, :] - self._observations[:, np.newaxis])**2 \\\n",
    "            / self._kernel_scale**2 / 2, axis=1) - np.log(self._n_samples))\n",
    "\n",
    "        return score\n",
    "\n",
    "    def n_parameters(self):\n",
    "        return 1\n",
    "\n",
    "    def get_parameter_names(self):\n",
    "        return ['Mean']\n",
    "\n",
    "    def get_id(self):\n",
    "        return [None]\n",
    "\n",
    "\n",
    "class KDEFilterLogLikelihood(chi.LogLikelihood):\n",
    "    def __init__(self, observations, n_samples=1000, kernel_scale=None):\n",
    "        self._observations = observations\n",
    "        self._n_obs = len(observations)\n",
    "        self._n_samples = int(n_samples)\n",
    "        self._normal = chi.GaussianErrorModel()\n",
    "\n",
    "        # Set kernel scale\n",
    "        # (If None or non-positive use Scott's rule of thumb (like scipy))\n",
    "        if (not kernel_scale) or (kernel_scale <=0):\n",
    "            kernel_scale = self._n_samples ** (-0.2)\n",
    "        self._kernel_scale = kernel_scale\n",
    "\n",
    "    def __call__(self, parameters):\n",
    "        standard_samples = parameters[:self._n_samples]\n",
    "        mean = parameters[-1]\n",
    "        sigma = 2\n",
    "        samples = mean + standard_samples * sigma\n",
    "\n",
    "        # Estimate log-likelihood as the mean across kdes\n",
    "        score = np.sum(scipy.special.logsumexp(\n",
    "            - np.log(2 * np.pi) / 2 - np.log(self._kernel_scale) - \\\n",
    "            (samples[np.newaxis, :] - self._observations[:, np.newaxis])**2 \\\n",
    "            / self._kernel_scale**2 / 2, axis=1) - np.log(self._n_samples))\n",
    "\n",
    "        return score\n",
    "\n",
    "    def evaluateS1(self, parameters):\n",
    "        standard_samples = np.array(parameters[:self._n_samples])\n",
    "        mean = parameters[-1]\n",
    "        std = 2\n",
    "        samples = mean + standard_samples * std\n",
    "\n",
    "        # Estimate log-likelihood as the mean across kdes\n",
    "        scores = \\\n",
    "            - np.log(2 * np.pi) / 2 - np.log(self._kernel_scale) - \\\n",
    "            (samples[np.newaxis, :] - self._observations[:, np.newaxis])**2 \\\n",
    "            / self._kernel_scale**2 / 2\n",
    "        score = np.sum(\n",
    "            scipy.special.logsumexp(scores, axis=1) - np.log(self._n_samples))\n",
    "\n",
    "        # Collect sensitivities\n",
    "        # p = log mean exp scores\n",
    "        # dp/dsamples = exp(score) / sum(exp(scores)) * dscore / dsamples\n",
    "        # dp/dmean = exp(score) / sum(exp(scores)) * dscore / dmu\n",
    "        softmax = \\\n",
    "            np.exp(scores - np.max(scores, axis=1)[:, np.newaxis]) / np.sum(\n",
    "            np.exp(scores - np.max(scores, axis=1)[:, np.newaxis]),\n",
    "            axis=1)[:, np.newaxis]\n",
    "\n",
    "        sensitivities = np.empty(shape=self.n_parameters())\n",
    "        sensitivities[:self._n_samples] = \\\n",
    "            - std / self._kernel_scale**2 * np.sum(\n",
    "                (samples[np.newaxis, :] - self._observations[:, np.newaxis]) *\n",
    "                softmax,\n",
    "                axis=0)\n",
    "        sensitivities[self._n_samples] = \\\n",
    "            - 1 / self._kernel_scale**2 * np.sum(\n",
    "                (samples[np.newaxis, :] - self._observations[:, np.newaxis]) *\n",
    "                softmax)\n",
    "\n",
    "        return score, sensitivities\n",
    "\n",
    "    def n_parameters(self):\n",
    "        return self._n_samples + 1\n",
    "\n",
    "    def get_parameter_names(self):\n",
    "        return [\n",
    "            'Sample' for _ in range(self._n_samples)] + ['Mean']\n",
    "\n",
    "    def get_id(self):\n",
    "        return ['ID %d' % _id for _id in range(self._n_samples)] + [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare log-likelihood for fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 1000\n",
    "\n",
    "# Compute exact log-likelihood\n",
    "scores_exact = np.empty(n_repeats)\n",
    "log_likelihood = GaussianLogLikelihood(observations=data)\n",
    "for idr in range(n_repeats):\n",
    "    scores_exact[idr] = log_likelihood(true_parameters)\n",
    "\n",
    "# Compute naive Gaussian KDE log-likelihood\n",
    "scores_naive = np.empty(n_repeats)\n",
    "log_likelihood = NaiveGaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=1000)\n",
    "for idr in range(n_repeats):\n",
    "    scores_naive[idr] = log_likelihood(true_parameters)\n",
    "\n",
    "# Compute Gaussian KDE log-likelihood\n",
    "scores = np.empty(n_repeats)\n",
    "n_samples = 1000\n",
    "parameters = np.hstack([\n",
    "    np.random.normal(size=n_samples), true_parameters])\n",
    "log_likelihood = GaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "for idr in range(n_repeats):\n",
    "    scores[idr] = log_likelihood(parameters)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(n_repeats),\n",
    "    y=scores_exact,\n",
    "    name='Exact'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(n_repeats),\n",
    "    y=scores_naive,\n",
    "    name='Naive Gaussian Filter (1000 samples)'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(n_repeats),\n",
    "    y=scores,\n",
    "    name='Gaussian Filter (1000 samples)'\n",
    "))\n",
    "fig.update_layout(\n",
    "    xaxis_title='Repeat',\n",
    "    yaxis_title='Log-likelihood'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exact likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = GaussianLogLikelihood(observations=data)\n",
    "log_prior = pints.GaussianLogPrior(mean=0, sd=4)\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "n_iterations = 10000\n",
    "exact_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 1000\n",
    "thinning = 10\n",
    "az.plot_trace(\n",
    "    exact_posterior_samples.sel(draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Gaussian filter likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 samples per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = NaiveGaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=10)\n",
    "log_prior = pints.GaussianLogPrior(mean=0, sd=4)\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_sampler(sampler=pints.MetropolisRandomWalkMCMC)\n",
    "controller.set_n_runs(1)\n",
    "controller._initial_params[0, 0] = 1.2\n",
    "n_iterations = 50000\n",
    "naive_10_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 1000\n",
    "thinning = 10\n",
    "az.plot_trace(\n",
    "    naive_10_posterior_samples.sel(draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 samples per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = NaiveGaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=100)\n",
    "log_prior = pints.GaussianLogPrior(mean=0, sd=4)\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_sampler(sampler=pints.MetropolisRandomWalkMCMC)\n",
    "controller.set_n_runs(1)\n",
    "controller._initial_params[0, 0] = 1.2\n",
    "n_iterations = 50000\n",
    "naive_100_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 1000\n",
    "thinning = 10\n",
    "az.plot_trace(\n",
    "    naive_100_posterior_samples.sel(draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 samples per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = NaiveGaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=1000)\n",
    "log_prior = pints.GaussianLogPrior(mean=0, sd=4)\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_sampler(sampler=pints.MetropolisRandomWalkMCMC)\n",
    "controller.set_n_runs(1)\n",
    "controller._initial_params[0, 0] = 1.2\n",
    "n_iterations = 50000\n",
    "naive_1000_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 1000\n",
    "thinning = 10\n",
    "az.plot_trace(\n",
    "    naive_1000_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gaussian filter likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "log_likelihood = GaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.ComposedLogPrior(*[\n",
    "    pints.GaussianLogPrior(mean=0, sd=1)] * n_samples + [\n",
    "    pints.GaussianLogPrior(mean=0, sd=4)]\n",
    ")\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.NoUTurnMCMC)\n",
    "n_iterations = 2000\n",
    "gaussian_filter_10_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 1000\n",
    "thinning = 1\n",
    "az.plot_trace(\n",
    "    gaussian_filter_10_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "log_likelihood = GaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.ComposedLogPrior(*[\n",
    "    pints.GaussianLogPrior(mean=0, sd=1)] * n_samples + [\n",
    "    pints.GaussianLogPrior(mean=0, sd=4)]\n",
    ")\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.NoUTurnMCMC)\n",
    "n_iterations = 1500\n",
    "gaussian_filter_100_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "az.plot_trace(\n",
    "    gaussian_filter_100_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "log_likelihood = GaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.ComposedLogPrior(*[\n",
    "    pints.GaussianLogPrior(mean=0, sd=1)] * n_samples + [\n",
    "    pints.GaussianLogPrior(mean=0, sd=4)]\n",
    ")\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.NoUTurnMCMC)\n",
    "n_iterations = 1500\n",
    "gaussian_filter_1000_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "az.plot_trace(\n",
    "    gaussian_filter_1000_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "log_likelihood = GaussianFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.ComposedLogPrior(*[\n",
    "    pints.GaussianLogPrior(mean=0, sd=1)] * n_samples + [\n",
    "    pints.GaussianLogPrior(mean=0, sd=4)]\n",
    ")\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.NoUTurnMCMC)\n",
    "n_iterations = 1500\n",
    "gaussian_filter_10000_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "az.plot_trace(\n",
    "    gaussian_filter_10000_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_filter_10000_posterior_samples.to_netcdf(\n",
    "    'derived_data/posteriors/gaussian_model_gaussian_filter_10000_samples.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive KDE inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "log_likelihood = NaiveKDEFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.GaussianLogPrior(mean=0, sd=4)\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.MetropolisRandomWalkMCMC)\n",
    "n_iterations = 200000\n",
    "naive_kde_filter_10_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 20000\n",
    "thinning = 1000\n",
    "az.plot_trace(\n",
    "    naive_kde_filter_10_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "log_likelihood = NaiveKDEFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.GaussianLogPrior(mean=0, sd=4)\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.MetropolisRandomWalkMCMC)\n",
    "n_iterations = 200000\n",
    "naive_kde_filter_100_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 20000\n",
    "thinning = 1000\n",
    "az.plot_trace(\n",
    "    naive_kde_filter_100_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "log_likelihood = NaiveKDEFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.GaussianLogPrior(mean=0, sd=4)\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.MetropolisRandomWalkMCMC)\n",
    "n_iterations = 50000\n",
    "naive_kde_filter_1000_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 1000\n",
    "thinning = 10\n",
    "az.plot_trace(\n",
    "    naive_kde_filter_1000_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. KDE filter inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "log_likelihood = KDEFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.ComposedLogPrior(*[\n",
    "    pints.GaussianLogPrior(mean=0, sd=1)] * n_samples + [\n",
    "    pints.GaussianLogPrior(mean=0, sd=4)]\n",
    ")\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.NoUTurnMCMC)\n",
    "n_iterations = 1500\n",
    "kde_filter_10_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "az.plot_trace(\n",
    "    kde_filter_10_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "log_likelihood = KDEFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.ComposedLogPrior(*[\n",
    "    pints.GaussianLogPrior(mean=0, sd=1)] * n_samples + [\n",
    "    pints.GaussianLogPrior(mean=0, sd=4)]\n",
    ")\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.NoUTurnMCMC)\n",
    "n_iterations = 1500\n",
    "kde_filter_100_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=True)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "az.plot_trace(\n",
    "    kde_filter_100_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "log_likelihood = KDEFilterLogLikelihood(\n",
    "    observations=data, n_samples=n_samples)\n",
    "log_prior = pints.ComposedLogPrior(*[\n",
    "    pints.GaussianLogPrior(mean=0, sd=1)] * n_samples + [\n",
    "    pints.GaussianLogPrior(mean=0, sd=4)]\n",
    ")\n",
    "log_posterior = chi.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "controller = chi.SamplingController(log_posterior)\n",
    "controller.set_n_runs(1)\n",
    "controller.set_parallel_evaluation(False)\n",
    "controller.set_sampler(pints.NoUTurnMCMC)\n",
    "n_iterations = 1500\n",
    "kde_filter_1000_posterior_samples = controller.run(\n",
    "    n_iterations=n_iterations, log_to_screen=True)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "az.plot_trace(\n",
    "    kde_filter_1000_posterior_samples.sel(\n",
    "        draw=slice(warmup, n_iterations, thinning)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_filter_1000_posterior_samples.to_netcdf(\n",
    "    'derived_data/posteriors/gaussian_model_kde_filter_1000_samples.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to analytic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true distribution\n",
    "n = len(data)\n",
    "mu_0 = 0\n",
    "var_0 = 16\n",
    "true_mu, true_std = true_parameters\n",
    "true_var = true_std ** 2\n",
    "posterior_var = var_0 * true_var / (true_var + n * var_0)\n",
    "posterior_mean = posterior_var * (\n",
    "    mu_0 / var_0 + n * np.mean(data) / true_var)\n",
    "mus = np.linspace(1, 1.6, num=200)\n",
    "true_pdf = scipy.stats.norm(posterior_mean, np.sqrt(posterior_var)).pdf(mus)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(\n",
    "    1, 3, figsize=(10, 5), sharey='row', sharex='row')\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
    "axes[0].set_xlim([posterior_mean - 1, posterior_mean + 1])\n",
    "axes[0].set_xlabel('Mu')\n",
    "axes[1].set_xlabel('Mu')\n",
    "axes[2].set_xlabel('Mu')\n",
    "colors = sns.color_palette()\n",
    "\n",
    "# Plot exact likelihood inference\n",
    "warmup = 1000\n",
    "thinning = 10\n",
    "mu_samples = exact_posterior_samples.sel(\n",
    "    draw=slice(warmup, n_iterations, thinning))['Mean'].values[0]\n",
    "sns.kdeplot(\n",
    "    x=mu_samples,\n",
    "    fill=True,\n",
    "    common_norm=False, alpha=.5, linewidth=1, ax=axes[0],\n",
    "    palette=colors[0], legend=False)\n",
    "\n",
    "# Plot Gaussian filter posteriors\n",
    "warmup = 1000\n",
    "thinning = 1\n",
    "mu_samples = gaussian_filter_10_posterior_samples.sel(\n",
    "    draw=slice(warmup, n_iterations, thinning))['Mean'].values[0]\n",
    "sns.kdeplot(\n",
    "    x=mu_samples,\n",
    "    fill=True,\n",
    "    common_norm=False, alpha=.5, linewidth=1, ax=axes[1],\n",
    "    palette=colors[0], legend=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "mu_samples = gaussian_filter_100_posterior_samples.sel(\n",
    "    draw=slice(warmup, n_iterations, thinning))['Mean'].values[0]\n",
    "sns.kdeplot(\n",
    "    x=mu_samples,\n",
    "    fill=True,\n",
    "    common_norm=False, alpha=.5, linewidth=1, ax=axes[1],\n",
    "    palette=colors[1], legend=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "mu_samples = gaussian_filter_1000_posterior_samples.sel(\n",
    "    draw=slice(warmup, n_iterations, thinning))['Mean'].values[0]\n",
    "sns.kdeplot(\n",
    "    x=mu_samples,\n",
    "    fill=True,\n",
    "    common_norm=False, alpha=.5, linewidth=1, ax=axes[1],\n",
    "    palette=colors[2], legend=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "mu_samples = gaussian_filter_10000_posterior_samples.sel(\n",
    "    draw=slice(warmup, n_iterations, thinning))['Mean'].values[0]\n",
    "sns.kdeplot(\n",
    "    x=mu_samples,\n",
    "    fill=True,\n",
    "    common_norm=False, alpha=.5, linewidth=1, ax=axes[1],\n",
    "    palette=colors[3], legend=False)\n",
    "\n",
    "# Plot KDE filter posteriors\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "mu_samples = kde_filter_10_posterior_samples.sel(\n",
    "    draw=slice(warmup, n_iterations, thinning))['Mean'].values[0]\n",
    "sns.kdeplot(\n",
    "    x=mu_samples,\n",
    "    fill=True,\n",
    "    common_norm=False, alpha=.5, linewidth=1, ax=axes[2],\n",
    "    palette=colors[0], legend=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "mu_samples = kde_filter_100_posterior_samples.sel(\n",
    "    draw=slice(warmup, n_iterations, thinning))['Mean'].values[0]\n",
    "sns.kdeplot(\n",
    "    x=mu_samples,\n",
    "    fill=True,\n",
    "    common_norm=False, alpha=.5, linewidth=1, ax=axes[2],\n",
    "    palette=colors[1], legend=False)\n",
    "\n",
    "warmup = 500\n",
    "thinning = 1\n",
    "mu_samples = kde_filter_1000_posterior_samples.sel(\n",
    "    draw=slice(warmup, n_iterations, thinning))['Mean'].values[0]\n",
    "sns.kdeplot(\n",
    "    x=mu_samples,\n",
    "    fill=True,\n",
    "    common_norm=False, alpha=.5, linewidth=1, ax=axes[2],\n",
    "    palette=colors[1], legend=False)\n",
    "\n",
    "# Overlay exact posterior\n",
    "axes[0].plot(\n",
    "    mus, true_pdf, color='black', linestyle='--', label='Analytic posterior')\n",
    "axes[1].plot(\n",
    "    mus, true_pdf, color='black', linestyle='--')\n",
    "axes[2].plot(\n",
    "    mus, true_pdf, color='black', linestyle='--')\n",
    "\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c903980782b3573d8df28ed0c764f6b9cec8453e6cc0f128c898eaf8d0e4b377"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
